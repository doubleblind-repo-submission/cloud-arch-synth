{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHMDbum3APd2"
      },
      "source": [
        "#Block C3: Evaluation of generated architectures (Bootstrap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss1u50LkARJ2",
        "outputId": "eee26768-8669-4450-8788-5e074ba28d1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Bootstrap .632 — 1 réplica ===\n",
            "Tamaño total: 396\n",
            "Train bootstrap (≈63% únicas + repetidas): 396\n",
            "OOB (no visto): 148\n",
            "Distribución por clase en OOB:\n",
            "tipo_arquitectura\n",
            "None        107\n",
            "Edge         35\n",
            "HPC           4\n",
            "Edge+HPC      2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\steve\\anaconda3\\envs\\rapids-24\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:161: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
            "  warnings.warn(\n",
            "C:\\Users\\steve\\anaconda3\\envs\\rapids-24\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:161: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Catálogo soporte=0.06  -> itemsets=117 | reglas=46\n",
            "  antecedents consequents   support  confidence      lift\n",
            "0    (Athena)        (S3)  0.093434         1.0  1.596774\n",
            "1      (Glue)        (S3)  0.083333         1.0  1.596774\n",
            "2  (RedShift)        (S3)  0.068182         1.0  1.596774\n",
            "\n",
            "Catálogo soporte=0.03  -> itemsets=338 | reglas=176\n",
            "  antecedents consequents   support  confidence      lift\n",
            "0    (Athena)        (S3)  0.093434         1.0  1.596774\n",
            "1      (Glue)        (S3)  0.083333         1.0  1.596774\n",
            "2  (RedShift)        (S3)  0.068182         1.0  1.596774\n",
            "\n",
            "Generadas (006): 148 | Generadas (003): 148 | OOB: 148\n",
            "\n",
            "— Comparación de tamaños OOB vs Generadas [soporte 0.06] —\n",
            "  KS D = 0.8311  |  p-value = 4.667e-52\n",
            "  Wasserstein = 4.4189\n",
            "  Real (mean±std) = 7.60±2.10 | Gen (mean±std) = 3.18±1.05\n",
            "\n",
            "— Comparación de tamaños OOB vs Generadas [soporte 0.03] —\n",
            "  KS D = 0.7838  |  p-value = 2.731e-45\n",
            "  Wasserstein = 4.2905\n",
            "  Real (mean±std) = 7.60±2.10 | Gen (mean±std) = 3.31±1.21\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# BLOCK C3 (part 1) — .632 Bootstrap: 1 replicate + initial validation\n",
        "# ------------------------------------------------------------\n",
        "# Goal:\n",
        "#   - Perform ONE stratified bootstrap replicate (≈63% train, ≈37% OOB).\n",
        "#   - Mine catalogs (FP-Growth) ONLY on the bootstrap (two supports: 0.06 and 0.03).\n",
        "#   - Generate |OOB| architectures using each catalog (configs '006' and '003').\n",
        "#   - Validate SIZE ONLY (#services) between OOB and generated:\n",
        "#       * KS test (D statistic and p-value)\n",
        "#       * Wasserstein (Earth Mover’s distance)\n",
        "# Requirements:\n",
        "#   - df_all in memory with columns ['services','tipo_arquitectura'] (from A0).\n",
        "#   - Function generate_architecture(seed, rules_df, frequent_itemsets, ...)\n",
        "#     defined in A2. If not defined, a minimal version is activated.\n",
        "#   - mlxtend installed (fpgrowth/association_rules).\n",
        "# Output:\n",
        "#   - Summary prints: set sizes, KS/Wasserstein per configuration.\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random, math\n",
        "from collections import Counter\n",
        "from sklearn.utils import resample\n",
        "from scipy.stats import ks_2samp, wasserstein_distance\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
        "\n",
        "# ---------- 0) Basic utilities ----------\n",
        "\n",
        "def _sizes_from_transactions(transactions):\n",
        "    \"\"\"Returns a list with the # of unique services per architecture.\"\"\"\n",
        "    return [len(set(tx)) for tx in transactions]\n",
        "\n",
        "def stratified_bootstrap(df, label_col='tipo_arquitectura', random_state=123):\n",
        "    \"\"\"\n",
        "    Stratified .632 bootstrap:\n",
        "      - Resamples WITH replacement WITHIN each class, preserving proportions.\n",
        "      - Returns df_boot (bootstrap train) and df_oob (out-of-bag, unseen).\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    parts = []\n",
        "    for clase, sub in df.groupby(label_col, dropna=False):\n",
        "        # resample with replacement to the same size of the stratum\n",
        "        idx_boot = resample(\n",
        "            sub.index, replace=True, n_samples=len(sub), random_state=rng\n",
        "        )\n",
        "        parts.append(df.loc[idx_boot])\n",
        "    df_boot = pd.concat(parts, axis=0)\n",
        "\n",
        "    # OOB: rows from the original that DO NOT appear in the bootstrap (as unique indices)\n",
        "    used = set(df_boot.index.unique())\n",
        "    all_idx = set(df.index)\n",
        "    oob_idx = sorted(list(all_idx - used))\n",
        "    df_oob = df.loc[oob_idx].copy()\n",
        "\n",
        "    return df_boot.reset_index(drop=True), df_oob.reset_index(drop=True)\n",
        "\n",
        "def mine_catalogs(transactions, supports=(0.06, 0.03), conf_min=0.60, lift_min=1.05):\n",
        "    \"\"\"\n",
        "    One-hot + FP-Growth for each support. Returns a dict:\n",
        "      catalogs[0.06] = (frequent_itemsets_df, rules_df)\n",
        "      catalogs[0.03] = (frequent_itemsets_df, rules_df)\n",
        "    \"\"\"\n",
        "    te = TransactionEncoder()\n",
        "    basket = te.fit(transactions).transform(transactions)\n",
        "    df_basket = pd.DataFrame(basket, columns=te.columns_).astype('uint8')\n",
        "\n",
        "    catalogs = {}\n",
        "    for s in supports:\n",
        "        fis = fpgrowth(df_basket, min_support=s, use_colnames=True)\n",
        "        rules = association_rules(fis, metric=\"confidence\", min_threshold=conf_min)\n",
        "        rules = rules[rules['lift'] > lift_min].copy()\n",
        "        rules.sort_values(['confidence','lift','support'], ascending=False, inplace=True)\n",
        "        rules.reset_index(drop=True, inplace=True)\n",
        "        catalogs[round(s, 2)] = (fis, rules)\n",
        "    return catalogs\n",
        "\n",
        "def select_seeds(df_boot, k, mix_ratio=0.7, random_state=123):\n",
        "    \"\"\"\n",
        "    Select k seeds for generation:\n",
        "      - mix_ratio * k: from the Top-10 most prevalent services in df_boot (one per seed)\n",
        "      - (1 - mix_ratio) * k: samples of real architectures from the boot (one row -> 1–2 services)\n",
        "    Leaves simple and reproducible traceability.\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    tx_boot = df_boot['services'].tolist()\n",
        "    # prevalences in boot\n",
        "    c = Counter()\n",
        "    for tx in tx_boot:\n",
        "        for s in set(tx):\n",
        "            c[s] += 1\n",
        "    top10 = [s for s,_ in c.most_common(10)] or ['S3']\n",
        "\n",
        "    n_top = int(round(k * mix_ratio))\n",
        "    n_real = k - n_top\n",
        "\n",
        "    seeds = []\n",
        "    # top-service seeds (unitary)\n",
        "    for _ in range(n_top):\n",
        "        seeds.append([rng.choice(top10)])\n",
        "    # seeds from boot architectures (take 1–2 random services from the row)\n",
        "    if len(df_boot) > 0:\n",
        "        idxs = rng.choice(len(df_boot), size=n_real, replace=True)\n",
        "        for i in idxs:\n",
        "            row = df_boot.iloc[i]\n",
        "            if len(row['services']) == 0:\n",
        "                seeds.append(['S3'])\n",
        "            else:\n",
        "                s_pick = rng.choice(row['services'], size=min(2, len(row['services'])), replace=False).tolist()\n",
        "                seeds.append(s_pick)\n",
        "    return seeds\n",
        "\n",
        "# ---------- 0-bis) Fallback: minimal generator (if A2 is not in memory) ----------\n",
        "if 'generate_architecture' not in globals():\n",
        "    def generate_architecture(seed, rules_df, frequent_itemsets,\n",
        "                              max_size=10, n_candidates=40, random_state=None):\n",
        "        \"\"\"Minimal version of the A2 generator (same weighting criterion).\"\"\"\n",
        "        if random_state is not None:\n",
        "            random.seed(random_state)\n",
        "        cur = set(seed)\n",
        "        for _ in range(n_candidates):\n",
        "            if len(cur) >= max_size:\n",
        "                break\n",
        "            applicable = rules_df[rules_df['antecedents'].apply(lambda x: x.issubset(cur))]\n",
        "            if not applicable.empty:\n",
        "                tmp = applicable.copy()\n",
        "                tmp['weight'] = tmp['confidence'] * tmp['support'] * tmp['lift'].map(lambda l: max(1e-6, math.log2(l)))\n",
        "                idx = random.choices(tmp.index, weights=tmp['weight'], k=1)[0]\n",
        "                cons = list(tmp.loc[idx, 'consequents'])\n",
        "                choice = random.choice(cons)\n",
        "            else:\n",
        "                # simple fallback: sample elements from frequent itemsets\n",
        "                pool = [i for s in frequent_itemsets['itemsets'] for i in s]\n",
        "                if not pool: break\n",
        "                choice = random.choice(pool)\n",
        "            cur.add(choice)\n",
        "        return sorted(cur)\n",
        "\n",
        "# ---------- 1) .632 Bootstrap — 1 replicate ----------\n",
        "\n",
        "df_boot, df_oob = stratified_bootstrap(df_all, label_col='tipo_arquitectura', random_state=2025)\n",
        "\n",
        "print(\"=== .632 Bootstrap — 1 replicate ===\")\n",
        "print(\"Total size:\", len(df_all))\n",
        "print(\"Bootstrap train (≈63% uniques + repeats):\", len(df_boot))\n",
        "print(\"OOB (unseen):\", len(df_oob))\n",
        "print(\"Class distribution in OOB:\")\n",
        "print(df_oob['tipo_arquitectura'].value_counts(dropna=False).to_string())\n",
        "\n",
        "# ---------- 2) Mining ONLY on bootstrap ----------\n",
        "\n",
        "tx_boot = df_boot['services'].tolist()\n",
        "catalogs = mine_catalogs(tx_boot, supports=(0.06, 0.03), conf_min=0.60, lift_min=1.05)\n",
        "\n",
        "for s, (fis, rules) in catalogs.items():\n",
        "    print(f\"\\nCatalog support={s:.2f}  -> itemsets={len(fis)} | rules={len(rules)}\")\n",
        "    print(rules[['antecedents','consequents','support','confidence','lift']].head(3))\n",
        "\n",
        "# ---------- 3) Generation (two configurations) ----------\n",
        "\n",
        "N_target = len(df_oob)                             # generate as many as OOB\n",
        "seeds = select_seeds(df_boot, k=N_target, mix_ratio=0.5, random_state=2025)\n",
        "\n",
        "def generate_batch(config_key):\n",
        "    fis, rules = catalogs[0.06] if config_key=='006' else catalogs[0.03]\n",
        "    gens = []\n",
        "    for i in range(N_target):\n",
        "        seed = seeds[i]\n",
        "        arch = generate_architecture(seed, rules, fis,\n",
        "                                     max_size=10, n_candidates=100, random_state=2025+i)\n",
        "        if arch:\n",
        "            gens.append(arch)\n",
        "    return gens\n",
        "\n",
        "gen_006 = generate_batch('006')\n",
        "gen_003 = generate_batch('003')\n",
        "\n",
        "print(f\"\\nGenerated (006): {len(gen_006)} | Generated (003): {len(gen_003)} | OOB: {N_target}\")\n",
        "\n",
        "# ---------- 4) Initial validation: size (#services) ----------\n",
        "\n",
        "tx_oob = df_oob['services'].tolist()\n",
        "\n",
        "def eval_size_metrics(real_tx, gen_tx, label):\n",
        "    real_sizes = _sizes_from_transactions(real_tx)\n",
        "    gen_sizes  = _sizes_from_transactions(gen_tx)\n",
        "    ks = ks_2samp(real_sizes, gen_sizes, alternative='two-sided', mode='auto')\n",
        "    wd = wasserstein_distance(real_sizes, gen_sizes)\n",
        "    print(f\"\\n— {label} —\")\n",
        "    print(f\"  KS D = {ks.statistic:.4f}  |  p-value = {ks.pvalue:.4g}\")\n",
        "    print(f\"  Wasserstein = {wd:.4f}\")\n",
        "    print(f\"  Real (mean±std) = {np.mean(real_sizes):.2f}±{np.std(real_sizes):.2f} | \"\n",
        "          f\"Gen (mean±std) = {np.mean(gen_sizes):.2f}±{np.std(gen_sizes):.2f}\")\n",
        "\n",
        "eval_size_metrics(tx_oob, gen_006, \"Size comparison OOB vs Generated [support 0.06]\")\n",
        "eval_size_metrics(tx_oob, gen_003, \"Size comparison OOB vs Generated [support 0.03]\")\n",
        "\n",
        "# (Optional) save seeds and first examples for paper traceability:\n",
        "# pd.DataFrame({'seed': ['|'.join(s) for s in seeds],\n",
        "#               'gen_006': ['|'.join(g) for g in gen_006[:10]],\n",
        "#               'gen_003': ['|'.join(g) for g in gen_003[:10]]}).to_csv(\"trace_bootstrap_replica1.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hH11PwcrJTi5"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PATCH G1 — Size-Aware Generator (aligns synthetic sizes with real ones)\n",
        "# ------------------------------------------------------------\n",
        "# Idea:\n",
        "#   1) Start with the architecture generated by rules (your current generate_architecture).\n",
        "#   2) Define a target_size ~ real size distribution (#services) from the fold/bootstrap.\n",
        "#   3) If the generated architecture is smaller, FILL it with plausible services:\n",
        "#        - sampled with probability proportional to the observed support (frequency) in train\n",
        "#        - without repeating already present services\n",
        "#        - respecting an optional \"forbidden\" set\n",
        "#        - with the option to require at least one Edge service if desired\n",
        "#   4) Retry with a different seed if the result is still too small.\n",
        "#\n",
        "# Requirements:\n",
        "#   - 'mine_catalogs' must return \"fis\" (frequent itemsets) and \"rules\".\n",
        "#   - fis is a DataFrame with columns: ['support','itemsets'] (from mlxtend).\n",
        "#   - df_train is the DF of the fold/bootstrap (to estimate real size distribution).\n",
        "#   - Your generate_architecture(seed, rules, fis, ...) must already exist.\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# --- (Optional) list of services considered \"Edge\"\n",
        "EDGE_SERVICES = set([\n",
        "    \"Snowball\",\"Snowcone\",\"Snowmobile\",\"SnowFamily\",\n",
        "    \"SageMakerNeo\",\"SageMakerEdgeManager\",\"Monitron\",\"Panorama\",\n",
        "    \"RoboMaker\",\"CloudFront\",\"Greengrass\",\"FreeRTOS\",\"IoTCore\",\"IoTSiteWise\",\n",
        "    \"AlexaForBusiness\",\"LocalZones\",\"Wavelength\",\"Outpost\",\"StorageGateway\",\n",
        "    \"UserConsumerCamera\",\"UserCompanyEdge\",\"UserConsumerEdge\",\n",
        "    \"UserConsumerIOT\",\"UserConsumerPOS\",\"UserCompanyDrone\",\"UserConsumerFarmer\",\n",
        "    \"UserConsumerAlexaGoogleHome\",\"UserCompanyElementalLiveDevice\",\"UserConsumerTV\",\n",
        "    \"LambdaAtEdge\"\n",
        "])\n",
        "\n",
        "def _real_size_sampler(df_train, rng, clip=(5, 11)):\n",
        "    \"\"\"\n",
        "    Estimate the distribution of real architecture sizes in 'df_train' and sample a target_size.\n",
        "    - By default, clip to [5, 11] (approx. between P25 and P75 of your dataset).\n",
        "    \"\"\"\n",
        "    sizes = [len(set(s)) for s in df_train[\"services\"]]\n",
        "    mu, sd = float(np.mean(sizes)), max(0.8, float(np.std(sizes, ddof=1)))\n",
        "    # Sample from a “soft” truncated normal distribution\n",
        "    for _ in range(10):\n",
        "        t = int(round(rng.normal(mu, sd)))\n",
        "        if clip[0] <= t <= clip[1]:\n",
        "            return t\n",
        "    # fallback: use the median if no valid sample is drawn in 10 attempts\n",
        "    return int(np.median(sizes))\n",
        "\n",
        "def _item_weights_from_fis(fis):\n",
        "    \"\"\"\n",
        "    Derive weights per service using the frequent itemsets.\n",
        "    - Use singleton support if available; otherwise, use the maximum support\n",
        "      of any itemset containing that service (a reasonable proxy).\n",
        "    \"\"\"\n",
        "    singletons = {list(it)[0]: sup for sup, it in zip(fis[\"support\"], fis[\"itemsets\"]) if len(it)==1}\n",
        "    if singletons:\n",
        "        return singletons\n",
        "    # If no singletons exist, build weights by aggregation\n",
        "    acc = Counter()\n",
        "    for sup, it in zip(fis[\"support\"], fis[\"itemsets\"]):\n",
        "        for s in it:\n",
        "            acc[s] = max(acc[s], sup)\n",
        "    return dict(acc)\n",
        "\n",
        "def _complete_with_items(current, weights, target_size, rng,\n",
        "                         forbid=frozenset(), require_edge=False):\n",
        "    \"\"\"\n",
        "    Completes 'current' up to target_size by selecting services via roulette proportional to weights.\n",
        "    - Avoids duplicates and forbidden services.\n",
        "    - If require_edge=True, ensures at least one service is in EDGE_SERVICES.\n",
        "    \"\"\"\n",
        "    chosen = set(current)\n",
        "    pool = [s for s in weights.keys() if s not in chosen and s not in forbid]\n",
        "    if not pool:\n",
        "        return list(chosen)\n",
        "\n",
        "    # Probability vector proportional to weights\n",
        "    w = np.array([weights[s] for s in pool], dtype=float)\n",
        "    if w.sum() <= 0:\n",
        "        w = np.ones_like(w)\n",
        "    p = w / w.sum()\n",
        "\n",
        "    # Fill until reaching the target size\n",
        "    while len(chosen) < target_size and len(pool) > 0:\n",
        "        s = rng.choice(pool, p=p)\n",
        "        chosen.add(s)\n",
        "        # remove from pool\n",
        "        idx = pool.index(s)\n",
        "        pool.pop(idx); p = np.delete(p, idx)\n",
        "        if p.sum() > 0:\n",
        "            p = p / p.sum()\n",
        "\n",
        "    # If require_edge=True and none exists, try replacing one non-Edge with an Edge service\n",
        "    if require_edge and not (EDGE_SERVICES & chosen):\n",
        "        candidates = [s for s in weights.keys() if s in EDGE_SERVICES and s not in chosen and s not in forbid]\n",
        "        if candidates:\n",
        "            # choose the most plausible Edge service by weight\n",
        "            s_edge = max(candidates, key=lambda s: weights.get(s, 0))\n",
        "            # replace the least weighted non-Edge service if necessary\n",
        "            non_edge = [s for s in chosen if s not in EDGE_SERVICES]\n",
        "            if non_edge:\n",
        "                s_out = min(non_edge, key=lambda s: weights.get(s, 0))\n",
        "                chosen.remove(s_out); chosen.add(s_edge)\n",
        "\n",
        "    return list(chosen)\n",
        "\n",
        "def generate_architecture_size_aware(seed, rules, fis, df_train,\n",
        "                                     max_size=10, n_candidates=100,\n",
        "                                     require_edge=False, random_state=2025):\n",
        "    \"\"\"\n",
        "    Size-aware wrapper:\n",
        "      1) Calls your 'generate_architecture' (rule-based).\n",
        "      2) If too small, fills with plausible services up to target_size.\n",
        "      3) Retries once with a different seed if still below a minimum size.\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    # 1) target_size guided by real size distribution\n",
        "    target_size = min(max_size, _real_size_sampler(df_train, rng, clip=(5, 11)))\n",
        "\n",
        "    # 2) attempt generation via rules\n",
        "    arch = generate_architecture(seed, rules, fis,\n",
        "                                 max_size=max_size, n_candidates=n_candidates,\n",
        "                                 random_state=random_state)\n",
        "\n",
        "    # 3) if too small, fill with plausible services\n",
        "    if arch is None: arch = []\n",
        "    weights = _item_weights_from_fis(fis)\n",
        "    arch_filled = _complete_with_items(arch, weights, target_size, rng,\n",
        "                                       forbid=frozenset(), require_edge=require_edge)\n",
        "\n",
        "    # 4) if (for any reason) still too short, retry once with noise added to the seed\n",
        "    MIN_SIZE = 5\n",
        "    if len(set(arch_filled)) < MIN_SIZE:\n",
        "        seed2 = list(seed) if isinstance(seed, (list, tuple)) else [seed]\n",
        "        arch2 = generate_architecture(seed2, rules, fis,\n",
        "                                      max_size=max_size, n_candidates=n_candidates,\n",
        "                                      random_state=random_state + 13)\n",
        "        arch_filled = _complete_with_items(arch2 or [], weights, target_size, rng,\n",
        "                                           forbid=frozenset(), require_edge=require_edge)\n",
        "\n",
        "    return list(dict.fromkeys(arch_filled))  # stable order without duplicates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxFIEAeOJTi6"
      },
      "outputs": [],
      "source": [
        "# === Utilities used by workers ===\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "from math import log\n",
        "\n",
        "def _service_universe(transactions):\n",
        "    u = set()\n",
        "    for t in transactions:\n",
        "        u.update(t)\n",
        "    return sorted(u)\n",
        "\n",
        "def _freq_vector(transactions, universe):\n",
        "    # OOV-safe version\n",
        "    counts = dict.fromkeys(universe, 0)\n",
        "    U = set(universe)\n",
        "    for t in transactions:\n",
        "        for s in set(t):\n",
        "            if s in U:\n",
        "                counts[s] += 1\n",
        "    v = np.array([counts[s] for s in universe], dtype=float)\n",
        "    v = v / max(1.0, v.sum())\n",
        "    return v\n",
        "\n",
        "def jensen_shannon(p, q, base=np.e):\n",
        "    p = np.asarray(p, dtype=float); q = np.asarray(q, dtype=float)\n",
        "    eps = 1e-12\n",
        "    p = np.maximum(p, eps); q = np.maximum(q, eps)\n",
        "    p = p / p.sum(); q = q / q.sum()\n",
        "    m = 0.5*(p+q)\n",
        "    return 0.5*entropy(p, m, base=base) + 0.5*entropy(q, m, base=base)\n",
        "\n",
        "def _cooccurrence_matrix(transactions, universe):\n",
        "    idx = {s:i for i,s in enumerate(universe)}\n",
        "    n = len(universe)\n",
        "    M = np.zeros((n, n), dtype=float)\n",
        "    for t in transactions:\n",
        "        tt = sorted({s for s in t if s in idx})\n",
        "        for i in range(len(tt)):\n",
        "            for j in range(i, len(tt)):\n",
        "                a = idx[tt[i]]; b = idx[tt[j]]\n",
        "                M[a,b] += 1.0\n",
        "                if a != b:\n",
        "                    M[b,a] += 1.0\n",
        "    m = max(1.0, len(transactions))\n",
        "    return M / m\n",
        "\n",
        "def frobenius_norm(A, B):\n",
        "    D = A - B\n",
        "    return float(np.sqrt((D*D).sum()))\n",
        "\n",
        "def _confidence_of_rule(rule, transactions):\n",
        "    ant = set(rule[0]); cons = set(rule[1])\n",
        "    if not ant: return np.nan\n",
        "    sup_ant = 0; sup_both = 0\n",
        "    for t in transactions:\n",
        "        T = set(t)\n",
        "        if ant.issubset(T):\n",
        "            sup_ant += 1\n",
        "            if cons.issubset(T):\n",
        "                sup_both += 1\n",
        "    if sup_ant == 0: return np.nan\n",
        "    return sup_both / sup_ant\n",
        "\n",
        "def rules_delta_confidence(rules_df, tx_test, tx_gen, top_k=50):\n",
        "    if len(rules_df)==0:\n",
        "        return dict(delta_mean=np.nan, delta_median=np.nan, delta_p95=np.nan, n_eval=0)\n",
        "    r = rules_df.sort_values('lift', ascending=False).head(top_k)\n",
        "    deltas = []\n",
        "    for _, row in r.iterrows():\n",
        "        ant = tuple(sorted(list(row['antecedents'])))\n",
        "        con = tuple(sorted(list(row['consequents'])))\n",
        "        c_test = _confidence_of_rule((ant, con), tx_test)\n",
        "        c_gen  = _confidence_of_rule((ant, con), tx_gen)\n",
        "        if np.isfinite(c_test) and np.isfinite(c_gen):\n",
        "            deltas.append(abs(c_test - c_gen))\n",
        "    if not deltas:\n",
        "        return dict(delta_mean=np.nan, delta_median=np.nan, delta_p95=np.nan, n_eval=0)\n",
        "    deltas = np.asarray(deltas, dtype=float)\n",
        "    return dict(delta_mean=float(deltas.mean()),\n",
        "                delta_median=float(np.median(deltas)),\n",
        "                delta_p95=float(np.percentile(deltas, 95)),\n",
        "                n_eval=int(len(deltas)))\n",
        "\n",
        "def jaccard(a, b):\n",
        "    A, B = set(a), set(b)\n",
        "    if not A and not B: return 1.0\n",
        "    return len(A & B) / max(1.0, len(A | B))\n",
        "\n",
        "def nn_jaccard_stats(tx_gen, tx_real):\n",
        "    if len(tx_gen)==0 or len(tx_real)==0:\n",
        "        return dict(mean=np.nan, median=np.nan, p95=np.nan)\n",
        "    vals = []\n",
        "    for g in tx_gen:\n",
        "        best = 0.0\n",
        "        for r in tx_real:\n",
        "            best = max(best, jaccard(g, r))\n",
        "        vals.append(best)\n",
        "    vals = np.asarray(vals, dtype=float)\n",
        "    return dict(mean=float(vals.mean()),\n",
        "                median=float(np.median(vals)),\n",
        "                p95=float(np.percentile(vals, 95)))\n",
        "\n",
        "def intra_jaccard_mean(tx):\n",
        "    n = len(tx)\n",
        "    if n < 2: return np.nan\n",
        "    s = 0.0; c = 0\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            s += jaccard(tx[i], tx[j]); c += 1\n",
        "    return float(s / c)\n",
        "\n",
        "def coverage_ratio(tx, universe):\n",
        "    seen = set()\n",
        "    for t in tx: seen.update(t)\n",
        "    return len(seen) / max(1.0, len(universe))\n",
        "\n",
        "def normalized_entropy(transactions, universe):\n",
        "    counts = dict.fromkeys(universe, 0)\n",
        "    U = set(universe)\n",
        "    for t in transactions:\n",
        "        for s in set(t):\n",
        "            if s in U:\n",
        "                counts[s] += 1\n",
        "    v = np.array([counts[s] for s in universe], dtype=float)\n",
        "    if v.sum() == 0: return np.nan\n",
        "    p = v / v.sum()\n",
        "    h = entropy(p, base=np.e)\n",
        "    return float(h / max(1.0, log(len(universe))))\n",
        "\n",
        "def _sizes_from_transactions(transactions):\n",
        "    return [len(set(t)) for t in transactions]\n",
        "\n",
        "def _agg_ci(values):\n",
        "    arr = np.array(values, dtype=float)\n",
        "    if arr.size == 0:\n",
        "        return {\"mean\": np.nan, \"p2_5\": np.nan, \"p97_5\": np.nan}\n",
        "    return {\n",
        "        \"mean\": float(arr.mean()),\n",
        "        \"p2_5\": float(np.percentile(arr, 2.5)),\n",
        "        \"p97_5\": float(np.percentile(arr, 97.5)),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQNFwqxFJTi6",
        "outputId": "8a0cb3dc-97e8-432b-f177-61cd8c63df03"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
            "[Parallel(n_jobs=20)]: Done   1 tasks      | elapsed:  1.0min\n",
            "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=20)]: Done  21 tasks      | elapsed:  2.4min\n",
            "[Parallel(n_jobs=20)]: Done  32 tasks      | elapsed:  2.6min\n",
            "[Parallel(n_jobs=20)]: Done  45 tasks      | elapsed:  4.0min\n",
            "[Parallel(n_jobs=20)]: Done  58 tasks      | elapsed:  4.2min\n",
            "[Parallel(n_jobs=20)]: Done  73 tasks      | elapsed:  5.6min\n",
            "[Parallel(n_jobs=20)]: Done  88 tasks      | elapsed:  7.0min\n",
            "[Parallel(n_jobs=20)]: Done 105 tasks      | elapsed:  8.4min\n",
            "[Parallel(n_jobs=20)]: Done 122 tasks      | elapsed:  9.8min\n",
            "[Parallel(n_jobs=20)]: Done 141 tasks      | elapsed: 11.0min\n",
            "[Parallel(n_jobs=20)]: Done 160 tasks      | elapsed: 11.9min\n",
            "[Parallel(n_jobs=20)]: Done 182 out of 200 | elapsed: 13.5min remaining:  1.3min\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Resumen Bootstrap .632 — B réplicas ===\n",
            "                          support_0.06  support_0.03\n",
            "gen_mean_mean                   7.6280        7.6380\n",
            "gen_mean_p2.5                   7.3241        7.3494\n",
            "gen_mean_p97.5                  7.9643        7.9654\n",
            "jsd_mean                        0.0767        0.0601\n",
            "jsd_p2.5                        0.0624        0.0484\n",
            "jsd_p97.5                       0.0927        0.0727\n",
            "frob_mean                       1.7485        1.5876\n",
            "frob_p2.5                       1.4253        1.2629\n",
            "frob_p97.5                      2.0532        1.8748\n",
            "rules_delta_mean_mean           0.2281        0.2821\n",
            "rules_delta_mean_p2.5           0.1453        0.1689\n",
            "rules_delta_mean_p97.5          0.3071        0.4864\n",
            "rules_delta_median_mean         0.2183        0.2448\n",
            "rules_delta_median_p2.5         0.1161        0.1186\n",
            "rules_delta_median_p97.5        0.3167        0.5000\n",
            "rules_delta_p95_mean            0.4880        0.6849\n",
            "rules_delta_p95_p2.5            0.3373        0.4351\n",
            "rules_delta_p95_p97.5           0.6471        1.0000\n",
            "nn_jacc_mean_mean               0.4823        0.4662\n",
            "nn_jacc_mean_p2.5               0.4582        0.4424\n",
            "nn_jacc_mean_p97.5              0.5045        0.4877\n",
            "nn_jacc_median_mean             0.4775        0.4566\n",
            "nn_jacc_median_p2.5             0.4545        0.4444\n",
            "nn_jacc_median_p97.5            0.5000        0.5000\n",
            "nn_jacc_p95_mean                0.6285        0.6143\n",
            "nn_jacc_p95_p2.5                0.5714        0.5714\n",
            "nn_jacc_p95_p97.5               0.6667        0.6667\n",
            "intra_jacc_mean_mean            0.2587        0.2388\n",
            "intra_jacc_mean_p2.5            0.2282        0.2019\n",
            "intra_jacc_mean_p97.5           0.2876        0.2680\n",
            "coverage_mean                   0.3961        0.4510\n",
            "coverage_p2.5                   0.3353        0.3860\n",
            "coverage_p97.5                  0.4527        0.5102\n",
            "entropy_norm_mean               0.6664        0.6990\n",
            "entropy_norm_p2.5               0.6473        0.6797\n",
            "entropy_norm_p97.5              0.6832        0.7161\n",
            "N_reps                        200.0000      200.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=20)]: Done 200 out of 200 | elapsed: 14.0min finished\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# BLOCK B1 — Bootstrap .632 (B replicas, parallel)\n",
        "# With full metrics (size, prevalence, co-occurrences,\n",
        "# rules, local realism, diversity).\n",
        "# ------------------------------------------------------------\n",
        "# Prerequisites:\n",
        "#   - df_all[['services','tipo_arquitectura']]\n",
        "#   - Predefined functions: mine_catalogs, select_seeds,\n",
        "#     generate_architecture_size_aware (PATCH G1),\n",
        "#     plus utilities (_sizes_from_transactions, _freq_vector, etc.).\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from joblib import Parallel, delayed\n",
        "from scipy.stats import ks_2samp, wasserstein_distance\n",
        "\n",
        "# ---------- Parameters ----------\n",
        "B = 200\n",
        "N_JOBS = 20\n",
        "SUPPORTS = (0.06, 0.03)\n",
        "CONF_MIN  = 0.60\n",
        "LIFT_MIN  = 1.05\n",
        "MAX_SIZE  = 10\n",
        "N_CANDS   = 100\n",
        "SEED_BASE = 8000   # different from CV to ensure independence\n",
        "\n",
        "# ============================================================\n",
        "# Single bootstrap replica function\n",
        "# ============================================================\n",
        "def run_bootstrap(rep_idx):\n",
        "    \"\"\"\n",
        "    Runs one .632 bootstrap replica:\n",
        "      1) Stratify by tipo_arquitectura → df_boot, df_oob\n",
        "      2) Mine catalogs on df_boot\n",
        "      3) Generate on df_oob (size-aware)\n",
        "      4) Compute metrics\n",
        "    Returns a list of dicts (one row per config).\n",
        "    \"\"\"\n",
        "    df_boot, df_oob = stratified_bootstrap(\n",
        "        df_all, label_col=\"tipo_arquitectura\", random_state=SEED_BASE + rep_idx\n",
        "    )\n",
        "    tx_test = df_oob[\"services\"].tolist()\n",
        "    n_test  = len(tx_test)\n",
        "    if n_test == 0:\n",
        "        return []\n",
        "\n",
        "    # catalog mining\n",
        "    catalogs = mine_catalogs(\n",
        "        df_boot[\"services\"].tolist(),\n",
        "        supports=SUPPORTS, conf_min=CONF_MIN, lift_min=LIFT_MIN\n",
        "    )\n",
        "    # universe for distributional metrics\n",
        "    universe = _service_universe(df_boot[\"services\"].tolist())\n",
        "\n",
        "    # seeds from bootstrap\n",
        "    seeds = select_seeds(df_boot, k=n_test, mix_ratio=0.5,\n",
        "                         random_state=SEED_BASE + rep_idx)\n",
        "\n",
        "    rows_this_rep = []\n",
        "    for cfg in (\"006\",\"003\"):\n",
        "        fis, rules = catalogs[0.06] if cfg==\"006\" else catalogs[0.03]\n",
        "\n",
        "        # size-aware generation\n",
        "        gen = []\n",
        "        for i in range(n_test):\n",
        "            arch = generate_architecture_size_aware(\n",
        "                seeds[i], rules, fis, df_boot,\n",
        "                max_size=MAX_SIZE, n_candidates=N_CANDS,\n",
        "                require_edge=False,\n",
        "                random_state=SEED_BASE + rep_idx*100000 + i\n",
        "            )\n",
        "            if arch: gen.append(arch)\n",
        "\n",
        "        # === METRICS ===\n",
        "        # Sizes\n",
        "        real_sizes = _sizes_from_transactions(tx_test)\n",
        "        gen_sizes  = _sizes_from_transactions(gen)\n",
        "        ks = ks_2samp(real_sizes, gen_sizes, alternative=\"two-sided\", mode=\"auto\")\n",
        "        wd = wasserstein_distance(real_sizes, gen_sizes)\n",
        "\n",
        "        # Prevalence — JSD\n",
        "        p_real = _freq_vector(tx_test, universe)\n",
        "        p_gen  = _freq_vector(gen,      universe)\n",
        "        jsd = float(jensen_shannon(p_real, p_gen, base=np.e))\n",
        "\n",
        "        # Co-occurrences\n",
        "        C_real = _cooccurrence_matrix(tx_test, universe)\n",
        "        C_gen  = _cooccurrence_matrix(gen,      universe)\n",
        "        frob = frobenius_norm(C_real, C_gen)\n",
        "\n",
        "        # Rules — Δ-confidence\n",
        "        delta_rules = rules_delta_confidence(rules, tx_test, gen, top_k=50)\n",
        "\n",
        "        # Local realism — NN-Jaccard\n",
        "        nn = nn_jaccard_stats(gen, tx_test)\n",
        "\n",
        "        # Diversity\n",
        "        intra = intra_jaccard_mean(gen)\n",
        "        cov   = coverage_ratio(gen, universe)\n",
        "        Hn    = normalized_entropy(gen, universe)\n",
        "\n",
        "        rows_this_rep.append({\n",
        "            \"replica\": rep_idx, \"config\": cfg,\n",
        "            \"ks_D\": ks.statistic, \"ks_p\": ks.pvalue,\n",
        "            \"wasserstein\": wd,\n",
        "            \"real_mean\": float(np.mean(real_sizes)),\n",
        "            \"gen_mean\":  float(np.mean(gen_sizes)),\n",
        "            \"jsd\": jsd, \"frob\": frob,\n",
        "            \"rules_delta_mean\": delta_rules[\"delta_mean\"],\n",
        "            \"rules_delta_median\": delta_rules[\"delta_median\"],\n",
        "            \"rules_delta_p95\": delta_rules[\"delta_p95\"],\n",
        "            \"rules_n_eval\": delta_rules[\"n_eval\"],\n",
        "            \"nn_jacc_mean\": nn[\"mean\"],\n",
        "            \"nn_jacc_median\": nn[\"median\"],\n",
        "            \"nn_jacc_p95\": nn[\"p95\"],\n",
        "            \"intra_jacc_mean\": intra,\n",
        "            \"coverage\": cov,\n",
        "            \"entropy_norm\": Hn,\n",
        "            \"n_test\": n_test, \"n_gen\": len(gen),\n",
        "        })\n",
        "    return rows_this_rep\n",
        "\n",
        "# ============================================================\n",
        "# Run all replicas in parallel\n",
        "# ============================================================\n",
        "results_nested = Parallel(n_jobs=N_JOBS, backend=\"loky\", verbose=10)(\n",
        "    delayed(run_bootstrap)(b) for b in range(B)\n",
        ")\n",
        "\n",
        "# flatten\n",
        "rows = [row for sublist in results_nested for row in sublist]\n",
        "df_boot = pd.DataFrame(rows)\n",
        "\n",
        "# ============================================================\n",
        "# Summary by configuration (mean + 95% CI)\n",
        "# ============================================================\n",
        "def summarize_cfg(df, cfg):\n",
        "    sub = df[df[\"config\"]==cfg]\n",
        "    def s(col):\n",
        "        a = sub[col].dropna().values\n",
        "        out = _agg_ci(a)\n",
        "        return pd.Series({\n",
        "            f\"{col}_mean\": out[\"mean\"],\n",
        "            f\"{col}_p2.5\": out[\"p2_5\"],\n",
        "            f\"{col}_p97.5\": out[\"p97_5\"],\n",
        "        })\n",
        "    cols = [\n",
        "        \"gen_mean\",\"jsd\",\"frob\",\n",
        "        \"rules_delta_mean\",\"rules_delta_median\",\"rules_delta_p95\",\n",
        "        \"nn_jacc_mean\",\"nn_jacc_median\",\"nn_jacc_p95\",\n",
        "        \"intra_jacc_mean\",\"coverage\",\"entropy_norm\"\n",
        "    ]\n",
        "    parts = [s(c) for c in cols]\n",
        "    parts.append(pd.Series({\"N_reps\": len(sub)}))\n",
        "    return pd.concat(parts)\n",
        "\n",
        "summary = pd.concat({\n",
        "    \"support_0.06\": summarize_cfg(df_boot, \"006\"),\n",
        "    \"support_0.03\": summarize_cfg(df_boot, \"003\"),\n",
        "}, axis=1)\n",
        "\n",
        "print(\"=== Bootstrap .632 summary — B replicas ===\")\n",
        "print(summary.round(4).to_string())\n",
        "\n",
        "# Optional: save\n",
        "df_boot.to_csv(\"bootstrap_B200_full_metrics.csv\", index=False)\n",
        "summary.round(4).to_csv(\"bootstrap_B200_full_summary.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahF0u5xUJTi7",
        "outputId": "b8d94c2a-b90d-4de8-8b67-6e8a917b17a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "101\n"
          ]
        }
      ],
      "source": [
        "# 1) From existing labels\n",
        "df_edge = df_all[df_all[\"tipo_arquitectura\"] == \"Edge\"].copy()\n",
        "\n",
        "# 2) (Alternative) If labels are not available, mark Edge by services:\n",
        "# edge_services = [...]  # your list of Edge services used earlier\n",
        "# df_edge = df_all[df_all[\"services\"].apply(lambda ss: any(s in edge_services for s in ss))].copy()\n",
        "\n",
        "print(len(df_edge))  # should return ~101\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgGo_CVHJTi8",
        "outputId": "3daf07ef-5a37-4474-da74-72e7aafef13b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
            "[Parallel(n_jobs=20)]: Done   1 tasks      | elapsed:   11.9s\n",
            "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:  1.0min\n",
            "[Parallel(n_jobs=20)]: Done  21 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=20)]: Done  32 tasks      | elapsed:  3.9min\n",
            "[Parallel(n_jobs=20)]: Done  45 tasks      | elapsed:  5.0min\n",
            "[Parallel(n_jobs=20)]: Done  58 tasks      | elapsed:  6.3min\n",
            "[Parallel(n_jobs=20)]: Done  73 tasks      | elapsed:  7.5min\n",
            "[Parallel(n_jobs=20)]: Done  88 tasks      | elapsed:  9.6min\n",
            "[Parallel(n_jobs=20)]: Done 105 tasks      | elapsed: 13.4min\n",
            "[Parallel(n_jobs=20)]: Done 122 tasks      | elapsed: 16.7min\n",
            "[Parallel(n_jobs=20)]: Done 141 tasks      | elapsed: 19.5min\n",
            "[Parallel(n_jobs=20)]: Done 160 tasks      | elapsed: 22.1min\n",
            "[Parallel(n_jobs=20)]: Done 182 out of 200 | elapsed: 24.5min remaining:  2.4min\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Resumen Bootstrap .632 (Edge) — B réplicas ===\n",
            "                          support_0.06  support_0.03\n",
            "gen_mean_mean                   8.3901        8.8878\n",
            "gen_mean_p2.5                   7.7812        8.1626\n",
            "gen_mean_p97.5                  9.1319        9.5386\n",
            "jsd_mean                        0.1121        0.1131\n",
            "jsd_p2.5                        0.0802        0.0802\n",
            "jsd_p97.5                       0.1552        0.1689\n",
            "frob_mean                       3.0163        3.1117\n",
            "frob_p2.5                       2.1525        2.2508\n",
            "frob_p97.5                      4.8103        4.3770\n",
            "rules_delta_mean_mean           0.4931        0.5841\n",
            "rules_delta_mean_p2.5           0.2433        0.0000\n",
            "rules_delta_mean_p97.5          0.9122        1.0000\n",
            "rules_delta_median_mean         0.4835        0.5907\n",
            "rules_delta_median_p2.5         0.1667        0.0000\n",
            "rules_delta_median_p97.5        1.0000        1.0000\n",
            "rules_delta_p95_mean            0.8882        0.8001\n",
            "rules_delta_p95_p2.5            0.5708        0.0000\n",
            "rules_delta_p95_p97.5           1.0000        1.0000\n",
            "nn_jacc_mean_mean               0.4356        0.4151\n",
            "nn_jacc_mean_p2.5               0.3870        0.3668\n",
            "nn_jacc_mean_p97.5              0.4882        0.4844\n",
            "nn_jacc_median_mean             0.4314        0.4102\n",
            "nn_jacc_median_p2.5             0.3845        0.3571\n",
            "nn_jacc_median_p97.5            0.5000        0.4615\n",
            "nn_jacc_p95_mean                0.5710        0.5523\n",
            "nn_jacc_p95_p2.5                0.4994        0.4615\n",
            "nn_jacc_p95_p97.5               0.6801        0.7278\n",
            "intra_jacc_mean_mean            0.3212        0.2893\n",
            "intra_jacc_mean_p2.5            0.2475        0.2177\n",
            "intra_jacc_mean_p97.5           0.4590        0.3868\n",
            "coverage_mean                   0.3953        0.4657\n",
            "coverage_p2.5                   0.3398        0.3960\n",
            "coverage_p97.5                  0.4556        0.5556\n",
            "entropy_norm_mean               0.6870        0.7200\n",
            "entropy_norm_p2.5               0.6381        0.6760\n",
            "entropy_norm_p97.5              0.7256        0.7630\n",
            "N_reps                        200.0000      200.0000\n",
            "Archivos guardados:\n",
            "- edge_bootstrap_B200_full_metrics.csv\n",
            "- edge_bootstrap_B200_full_summary.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=20)]: Done 200 out of 200 | elapsed: 55.3min finished\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# BLOCK B1-EDGE — Bootstrap .632 (B replicas, parallel)\n",
        "# Subset: Edge architectures (df_edge)\n",
        "# Metrics: size, prevalence (JSD), co-occurrence (Frobenius),\n",
        "#          rules (Δ-confidence), local realism (NN-Jaccard),\n",
        "#          diversity (intra-Jaccard, coverage, entropy).\n",
        "# ------------------------------------------------------------\n",
        "# Environment prerequisites:\n",
        "#   - df_edge[['services']]  (101 Edge architectures)\n",
        "#   - Utility functions already defined in the notebook:\n",
        "#       mine_catalogs, select_seeds, generate_architecture_size_aware,\n",
        "#       _sizes_from_transactions, _service_universe, _freq_vector,\n",
        "#       jensen_shannon, _cooccurrence_matrix, frobenius_norm,\n",
        "#       rules_delta_confidence, nn_jaccard_stats, intra_jaccard_mean,\n",
        "#       coverage_ratio, normalized_entropy, _agg_ci\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from joblib import Parallel, delayed\n",
        "from scipy.stats import ks_2samp, wasserstein_distance\n",
        "\n",
        "# ---------- Parameters ----------\n",
        "B = 200\n",
        "N_JOBS   = 20\n",
        "SUPPORTS = (0.06, 0.03)\n",
        "CONF_MIN = 0.60\n",
        "LIFT_MIN = 1.05\n",
        "MAX_SIZE = 10\n",
        "N_CANDS  = 100\n",
        "SEED_BASE = 9100         # different from other experiments\n",
        "REQUIRE_EDGE = True      # enforce at least one edge service in generated architectures\n",
        "\n",
        "# ---------- Bootstrap .632 on a single stratum (Edge) ----------\n",
        "def bootstrap_split_edge(df, random_state=None):\n",
        "    \"\"\"\n",
        "    Return (df_boot, df_oob) for a .632 bootstrap:\n",
        "      - df_boot: resampled dataset with replacement (size N)\n",
        "      - df_oob : out-of-bag observations (≈ 36.8% expected)\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    n = len(df)\n",
        "    idx_boot = rng.randint(0, n, size=n)          # resample with replacement\n",
        "    boot = df.iloc[idx_boot].copy()\n",
        "\n",
        "    # oob = observations NOT present in idx_boot\n",
        "    mask_seen = np.zeros(n, dtype=bool)\n",
        "    mask_seen[np.unique(idx_boot)] = True\n",
        "    oob = df.iloc[~mask_seen].copy()\n",
        "    return boot, oob\n",
        "\n",
        "# ---------- One bootstrap replica (Edge) ----------\n",
        "def run_bootstrap_edge(rep_idx):\n",
        "    \"\"\"\n",
        "    Steps:\n",
        "      1) Perform .632 bootstrap on df_edge -> df_boot, df_oob\n",
        "      2) Mine frequent itemsets in df_boot (supports 0.06 and 0.03)\n",
        "      3) Generate |OOB| architectures using the size-aware generator\n",
        "      4) Compute metrics and return 2 rows (configs 006 and 003)\n",
        "    \"\"\"\n",
        "    df_boot, df_oob = bootstrap_split_edge(df_edge, random_state=SEED_BASE + rep_idx)\n",
        "    tx_test = df_oob[\"services\"].tolist()\n",
        "    n_test  = len(tx_test)\n",
        "    if n_test == 0:\n",
        "        return []\n",
        "\n",
        "    # Frequent itemset mining (Edge) on the bootstrap sample\n",
        "    catalogs = mine_catalogs(\n",
        "        df_boot[\"services\"].tolist(),\n",
        "        supports=SUPPORTS, conf_min=CONF_MIN, lift_min=LIFT_MIN\n",
        "    )\n",
        "\n",
        "    # Service universe for JSD / co-occurrence metrics\n",
        "    universe = _service_universe(df_boot[\"services\"].tolist())\n",
        "\n",
        "    # Seeds (50% real + 50% hybrid), drawn from df_boot\n",
        "    seeds = select_seeds(df_boot, k=n_test, mix_ratio=0.5,\n",
        "                         random_state=SEED_BASE + rep_idx)\n",
        "\n",
        "    rows_this_rep = []\n",
        "    for cfg in (\"006\", \"003\"):\n",
        "        fis, rules = catalogs[0.06] if cfg == \"006\" else catalogs[0.03]\n",
        "\n",
        "        # ---- Size-aware generation for |OOB|\n",
        "        gen = []\n",
        "        for i in range(n_test):\n",
        "            arch = generate_architecture_size_aware(\n",
        "                seeds[i], rules, fis, df_boot,\n",
        "                max_size=MAX_SIZE, n_candidates=N_CANDS,\n",
        "                require_edge=REQUIRE_EDGE,                  # <- key for Edge synthesis\n",
        "                random_state=SEED_BASE + rep_idx*100000 + i\n",
        "            )\n",
        "            if arch:\n",
        "                gen.append(arch)\n",
        "\n",
        "        # ====== METRICS ======\n",
        "        # Size comparison (additional KS/Wasserstein)\n",
        "        real_sizes = _sizes_from_transactions(tx_test)\n",
        "        gen_sizes  = _sizes_from_transactions(gen)\n",
        "        ks = ks_2samp(real_sizes, gen_sizes, alternative=\"two-sided\", mode=\"auto\")\n",
        "        wd = wasserstein_distance(real_sizes, gen_sizes)\n",
        "\n",
        "        # Prevalence — JSD\n",
        "        p_real = _freq_vector(tx_test, universe)\n",
        "        p_gen  = _freq_vector(gen,      universe)\n",
        "        jsd = float(jensen_shannon(p_real, p_gen, base=np.e))\n",
        "\n",
        "        # Co-occurrence — Frobenius norm\n",
        "        C_real = _cooccurrence_matrix(tx_test, universe)\n",
        "        C_gen  = _cooccurrence_matrix(gen,      universe)\n",
        "        frob = frobenius_norm(C_real, C_gen)\n",
        "\n",
        "        # Rules — Δ-confidence (evaluates top-K mined rules)\n",
        "        delta_rules = rules_delta_confidence(rules, tx_test, gen, top_k=50)\n",
        "\n",
        "        # Local realism — NN-Jaccard\n",
        "        nn = nn_jaccard_stats(gen, tx_test)\n",
        "\n",
        "        # Diversity\n",
        "        intra = intra_jaccard_mean(gen)\n",
        "        cov   = coverage_ratio(gen, universe)\n",
        "        Hn    = normalized_entropy(gen, universe)\n",
        "\n",
        "        rows_this_rep.append({\n",
        "            \"replica\": rep_idx, \"config\": cfg,\n",
        "            \"ks_D\": ks.statistic, \"ks_p\": ks.pvalue, \"wasserstein\": wd,\n",
        "            \"real_mean\": float(np.mean(real_sizes)),\n",
        "            \"gen_mean\":  float(np.mean(gen_sizes)),\n",
        "            \"jsd\": jsd, \"frob\": frob,\n",
        "            \"rules_delta_mean\": delta_rules[\"delta_mean\"],\n",
        "            \"rules_delta_median\": delta_rules[\"delta_median\"],\n",
        "            \"rules_delta_p95\": delta_rules[\"delta_p95\"],\n",
        "            \"rules_n_eval\": delta_rules[\"n_eval\"],\n",
        "            \"nn_jacc_mean\": nn[\"mean\"],\n",
        "            \"nn_jacc_median\": nn[\"median\"],\n",
        "            \"nn_jacc_p95\": nn[\"p95\"],\n",
        "            \"intra_jacc_mean\": intra,\n",
        "            \"coverage\": cov,\n",
        "            \"entropy_norm\": Hn,\n",
        "            \"n_test\": n_test, \"n_gen\": len(gen),\n",
        "        })\n",
        "    return rows_this_rep\n",
        "\n",
        "# ---------- Run in parallel ----------\n",
        "results_nested = Parallel(n_jobs=N_JOBS, backend=\"loky\", verbose=10)(\n",
        "    delayed(run_bootstrap_edge)(b) for b in range(B)\n",
        ")\n",
        "\n",
        "# Flatten and build final metrics DataFrame (per replica/config)\n",
        "rows = [row for sublist in results_nested for row in sublist]\n",
        "df_boot_edge = pd.DataFrame(rows)\n",
        "\n",
        "# ---------- Summary (mean + 95% CI) per configuration ----------\n",
        "def summarize_cfg(df, cfg):\n",
        "    sub = df[df[\"config\"] == cfg]\n",
        "    def s(col):\n",
        "        a = sub[col].dropna().values\n",
        "        out = _agg_ci(a)\n",
        "        return pd.Series({\n",
        "            f\"{col}_mean\":  out[\"mean\"],\n",
        "            f\"{col}_p2.5\":  out[\"p2_5\"],\n",
        "            f\"{col}_p97.5\": out[\"p97_5\"],\n",
        "        })\n",
        "    cols = [\n",
        "        \"gen_mean\",\"jsd\",\"frob\",\n",
        "        \"rules_delta_mean\",\"rules_delta_median\",\"rules_delta_p95\",\n",
        "        \"nn_jacc_mean\",\"nn_jacc_median\",\"nn_jacc_p95\",\n",
        "        \"intra_jacc_mean\",\"coverage\",\"entropy_norm\"\n",
        "    ]\n",
        "    parts = [s(c) for c in cols]\n",
        "    parts.append(pd.Series({\"N_reps\": len(sub)}))\n",
        "    return pd.concat(parts)\n",
        "\n",
        "summary_edge = pd.concat({\n",
        "    \"support_0.06\": summarize_cfg(df_boot_edge, \"006\"),\n",
        "    \"support_0.03\": summarize_cfg(df_boot_edge, \"003\"),\n",
        "}, axis=1)\n",
        "\n",
        "print(\"=== Bootstrap .632 (Edge) — B replicas summary ===\")\n",
        "print(summary_edge.round(4).to_string())\n",
        "\n",
        "# Save results (for paper appendices)\n",
        "df_boot_edge.to_csv(\"edge_bootstrap_B200_full_metrics.csv\", index=False)\n",
        "summary_edge.round(4).to_csv(\"edge_bootstrap_B200_full_summary.csv\")\n",
        "print(\"Files saved:\\n- edge_bootstrap_B200_full_metrics.csv\\n- edge_bootstrap_B200_full_summary.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python (rapids-24)",
      "language": "python",
      "name": "rapids-24"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}